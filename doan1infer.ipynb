{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":390390,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":321548,"modelId":342154},{"sourceId":391236,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":322146,"modelId":342823},{"sourceId":391306,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":322199,"modelId":342885}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install influxdb-client\n!pip install python-telegram-bot==13.7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:38:45.524151Z","iopub.execute_input":"2025-05-14T05:38:45.524383Z","iopub.status.idle":"2025-05-14T05:38:54.208802Z","shell.execute_reply.started":"2025-05-14T05:38:45.524365Z","shell.execute_reply":"2025-05-14T05:38:54.207895Z"}},"outputs":[{"name":"stdout","text":"Collecting influxdb-client\n  Downloading influxdb_client-1.48.0-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting reactivex>=4.0.4 (from influxdb-client)\n  Downloading reactivex-4.0.4-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from influxdb-client) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from influxdb-client) (2.9.0.post0)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from influxdb-client) (75.2.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from influxdb-client) (2.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->influxdb-client) (1.17.0)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from reactivex>=4.0.4->influxdb-client) (4.13.2)\nDownloading influxdb_client-1.48.0-py3-none-any.whl (746 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.2/746.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading reactivex-4.0.4-py3-none-any.whl (217 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: reactivex, influxdb-client\nSuccessfully installed influxdb-client-1.48.0 reactivex-4.0.4\nCollecting python-telegram-bot==13.7\n  Downloading python_telegram_bot-13.7-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.7) (2025.4.26)\nRequirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.7) (6.4.2)\nCollecting APScheduler==3.6.3 (from python-telegram-bot==13.7)\n  Downloading APScheduler-3.6.3-py2.py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.7) (2025.2)\nCollecting cachetools==4.2.2 (from python-telegram-bot==13.7)\n  Downloading cachetools-4.2.2-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (75.2.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (1.17.0)\nRequirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (5.3.1)\nDownloading python_telegram_bot-13.7-py3-none-any.whl (490 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-4.2.2-py3-none-any.whl (11 kB)\nInstalling collected packages: cachetools, APScheduler, python-telegram-bot\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 5.5.2\n    Uninstalling cachetools-5.5.2:\n      Successfully uninstalled cachetools-5.5.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed APScheduler-3.6.3 cachetools-4.2.2 python-telegram-bot-13.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom influxdb_client import InfluxDBClient\nimport tensorflow as tf\nimport joblib\nfrom sklearn.preprocessing import MinMaxScaler\nfrom telegram.ext import Updater, CommandHandler\nimport logging\nimport os\nimport errno\nimport json\nfrom tensorflow.keras import backend as K\n\n# Configure logging\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Model paths for pre-trained forecasting models\n# Update this path to where you placed the models on your local machine or server\nMODEL_DIR = '/kaggle/input/airpredict5/keras/default/1/'  # Replace with your actual path, e.g., '/home/user/models/'\nMODEL_PATHS = {\n    'temperature': os.path.join(MODEL_DIR, 'model_temperature.h5'),\n    'humidity': os.path.join(MODEL_DIR, 'model_humidity.h5'),\n    'co_ppm': os.path.join(MODEL_DIR, 'model_co_ppm.h5'),\n    'dust_density': os.path.join(MODEL_DIR, 'model_dust_density.h5')\n}\n\n# Global model cache for forecasting models\nMODELS = None\n\n# Define KLLossLayer without serialization registration\nclass KLLossLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n        self.add_loss(tf.reduce_mean(kl_loss))\n        return inputs\n\n    def get_config(self):\n        config = super(KLLossLayer, self).get_config()\n        return config\n\ndef check_training_status():\n    \"\"\"Check if models are ready or if training is in progress.\"\"\"\n    for model_name, path in MODEL_PATHS.items():\n        if not os.path.exists(path):\n            logger.warning(f\"Model file not found: {path}\")\n            return False, f\"Model file for {model_name} is missing. Training may be in progress.\"\n        try:\n            with open(path, 'rb'):\n                pass\n        except IOError as e:\n            if e.errno in (errno.EACCES, errno.EBUSY):\n                logger.warning(f\"Model file is locked: {path}\")\n                return False, f\"Model file for {model_name} is locked. Training may be in progress.\"\n            raise\n    return True, \"\"\n\ndef connect_to_influxdb():\n    \"\"\"Initialize connection to InfluxDB.\"\"\"\n    client = InfluxDBClient(\n        url=\"https://us-east-1-1.aws.cloud2.influxdata.com\",\n        token=\"rNKmS3Z7-_pqMbloCDVLeWJBaQq5AnflUidBT17ahV5kKePZbstxNxjLlr6kxegIdv0-HP6PSUG0N5QQ5_d0iA==\",\n        org=\"f4ea8e890f77d114\"\n    )\n    return client.query_api()\n\ndef fetch_data(query_api):\n    \"\"\"Fetch data from InfluxDB using the provided query.\"\"\"\n    query = '''\n    from(bucket: \"ESP32\")\n      |> range(start: -7h)\n      |> filter(fn: (r) => r._measurement == \"environment_data\")\n      |> filter(fn: (r) => r._field != \"co2_ppm\")\n    '''\n    tables = query_api.query(query)\n    return tables\n\ndef process_influxdb_data(tables):\n    \"\"\"Convert InfluxDB data to a pivoted DataFrame.\"\"\"\n    records = []\n    for table in tables:\n        for record in table.records:\n            records.append({\n                \"time\": record.get_time(),\n                \"field\": record.get_field(),\n                \"value\": record.get_value(),\n                \"location\": record.values.get(\"location\", None)\n            })\n\n    df = pd.DataFrame(records)\n    df = df.pivot_table(\n        index=[\"time\", \"location\"],\n        columns=\"field\",\n        values=\"value\"\n    ).reset_index()\n    return df\n\ndef preprocess_dataframe(df):\n    \"\"\"Preprocess the DataFrame with timezone conversion, sorting, and feature engineering.\"\"\"\n    # Convert timezone and sort data\n    df['time'] = pd.to_datetime(df['time']).dt.tz_convert('Asia/Ho_Chi_Minh')\n    df = df.sort_values(by='time', ascending=False).head(300).sort_values(by='time')\n\n    # Create date and time features\n    df['date'] = df['time'].dt.date\n    df['hour'] = df['time'].dt.hour\n    df['minute'] = df['time'].dt.minute\n     \n    # Interpolate missing values\n    df['temperature'] = df['temperature'].interpolate().ffill().bfill()\n    df['humidity'] = df['humidity'].interpolate().ffill().bfill()\n\n    # Add location-based features\n    df['Proximity_to_Industrial_Areas'] = df['location'].apply(lambda loc: 5 if loc == 'SPKT' else np.nan)\n    df['Population_Density'] = df['location'].apply(lambda loc: 3500 if loc == 'SPKT' else np.nan)\n\n    # Normalize temperature and humidity for CO generation\n    temp_min, temp_max = df['temperature'].min(), df['temperature'].max()\n    humid_min, humid_max = df['humidity'].min(), df['humidity'].max()\n    temp_norm = (df['temperature'] - temp_min) / (temp_max - temp_min)\n    humid_norm = (df['humidity'] - humid_min) / (humid_max - humid_min)\n\n    # Generate synthetic CO data\n    n = len(df)\n    a, b, c = 1.5, 0.5, 0.3\n    noise = np.random.normal(0, 0.15, n)\n    extra_noise = np.random.uniform(-0.1, 0.1, n)\n    fake_data = a * (temp_norm ** 2) + b * temp_norm - c * humid_norm + noise + extra_noise\n\n    # Normalize and skew the CO data\n    fake_data = (fake_data - fake_data.min()) / (fake_data.max() - fake_data.min())\n    fake_data = np.power(fake_data, 2.)\n\n    # Handle original CO data\n    if 'co_ppm' not in df.columns:\n        df['co_ppm'] = np.zeros(n)\n    original_co = df['co_ppm'].values\n\n    if np.all(original_co == 0):\n        normalized_co = np.zeros_like(original_co)\n    else:\n        normalized_co = (original_co - original_co.min()) / (original_co.max() - original_co.min())\n    combined_data = normalized_co * 0.5 + fake_data * 0.5\n\n    min_val, max_val = 0.65, 3.72\n    scaled_data = (combined_data - combined_data.min()) / (combined_data.max() - combined_data.min())\n    scaled_data = min_val + (max_val - min_val) * scaled_data\n    scaled_data = np.clip(scaled_data, min_val, max_val)\n\n    df['co_ppm'] = scaled_data\n\n    # Process dust_density\n    df['dust_density'] = df['dust_density'].replace(0, np.nan)\n    df['dust_density'] = df['dust_density'].interpolate().ffill().bfill()\n\n    # Select relevant columns\n    df = df[[\n        'time', 'date', 'hour', 'minute', 'location',\n        'temperature', 'humidity', 'co_ppm', 'dust_density',\n        'Proximity_to_Industrial_Areas', 'Population_Density'\n    ]]\n\n    return df\n\ndef load_models():\n    \"\"\"Load pre-trained TensorFlow models.\"\"\"\n    model_temperature = tf.keras.models.load_model(MODEL_PATHS['temperature'])\n    model_humidity = tf.keras.models.load_model(MODEL_PATHS['humidity'])\n    model_co_ppm = tf.keras.models.load_model(MODEL_PATHS['co_ppm'])\n    model_dust_density = tf.keras.models.load_model(MODEL_PATHS['dust_density'])\n    return model_temperature, model_humidity, model_co_ppm, model_dust_density\n\ndef load_models_once():\n    \"\"\"Load models once and cache them globally.\"\"\"\n    global MODELS\n    if MODELS is None:\n        MODELS = load_models()\n    return MODELS\n\ndef scale_features(df):\n    \"\"\"Scale features using MinMaxScaler.\"\"\"\n    scaler_temperature = MinMaxScaler()\n    scaler_humidity = MinMaxScaler()\n    scaler_co_ppm = MinMaxScaler()\n    scaler_dust_density = MinMaxScaler()\n\n    scaler_temperature.fit(df[['temperature']].values)\n    scaler_humidity.fit(df[['humidity']].values)\n    scaler_co_ppm.fit(df[['co_ppm']].values)\n    scaler_dust_density.fit(df[['dust_density']].values)\n\n    scaled_temperature = scaler_temperature.transform(df[['temperature']].values)\n    scaled_humidity = scaler_humidity.transform(df[['humidity']].values)\n    scaled_co_ppm = scaler_co_ppm.transform(df[['co_ppm']].values)\n    scaled_dust_density = scaler_dust_density.transform(df[['dust_density']].values)\n\n    return (scaler_temperature, scaler_humidity, scaler_co_ppm, scaler_dust_density,\n            scaled_temperature, scaled_humidity, scaled_co_ppm, scaled_dust_density)\n\ndef predict_next_timesteps(models, scalers, scaled_data, sequence_length=300, forecast_steps=60):\n    model_temperature, model_humidity, model_co_ppm, model_dust_density = models\n    scaler_temperature, scaler_humidity, scaler_co_ppm, scaler_dust_density = scalers[:4]\n    scaled_temperature, scaled_humidity, scaled_co_ppm, scaled_dust_density = scaled_data\n\n    current_temperature_seq = scaled_temperature[-sequence_length:].reshape(1, sequence_length, 1)\n    current_humidity_seq = scaled_humidity[-sequence_length:].reshape(1, sequence_length, 1)\n    current_co_ppm_seq = scaled_co_ppm[-sequence_length:].reshape(1, sequence_length, 1)\n    current_dust_density_seq = scaled_dust_density[-sequence_length:].reshape(1, sequence_length, 1)\n\n    predictions = []\n    for timestep in range(sequence_length, sequence_length + forecast_steps):\n        temperature_pred = model_temperature.predict(current_temperature_seq, verbose=0)\n        humidity_pred = model_humidity.predict(current_humidity_seq, verbose=0)\n        co_ppm_pred = model_co_ppm.predict(current_co_ppm_seq, verbose=0)\n        dust_density_pred = model_dust_density.predict(current_dust_density_seq, verbose=0)\n\n        temperature_pred = scaler_temperature.inverse_transform(temperature_pred)[0][0]\n        humidity_pred = scaler_humidity.inverse_transform(humidity_pred)[0][0]\n        co_ppm_pred = scaler_co_ppm.inverse_transform(co_ppm_pred)[0][0]\n        dust_density_pred = scaler_dust_density.inverse_transform(dust_density_pred)[0][0]\n\n        predictions.append({\n            'timestep': timestep,\n            'temperature': temperature_pred,\n            'humidity': humidity_pred,\n            'co_ppm': co_ppm_pred,\n            'dust_density': dust_density_pred\n        })\n\n        scaled_temperature_pred = scaler_temperature.transform([[temperature_pred]])[0][0]\n        scaled_humidity_pred = scaler_humidity.transform([[humidity_pred]])[0][0]\n        scaled_co_ppm_pred = scaler_co_ppm.transform([[co_ppm_pred]])[0][0]\n        scaled_dust_density_pred = scaler_dust_density.transform([[dust_density_pred]])[0][0]\n\n        current_temperature_seq = np.roll(current_temperature_seq, -1, axis=1)\n        current_temperature_seq[0, -1, 0] = scaled_temperature_pred\n        current_humidity_seq = np.roll(current_humidity_seq, -1, axis=1)\n        current_humidity_seq[0, -1, 0] = scaled_humidity_pred\n        current_co_ppm_seq = np.roll(current_co_ppm_seq, -1, axis=1)\n        current_co_ppm_seq[0, -1, 0] = scaled_co_ppm_pred\n        current_dust_density_seq = np.roll(current_dust_density_seq, -1, axis=1)\n        current_dust_density_seq[0, -1, 0] = scaled_dust_density_pred\n\n    return predictions\n\ndef prepare_rf_input(pred):\n    \"\"\"Prepare input vector for VAE-based prediction.\"\"\"\n    X_60min = np.array([\n        pred['temperature'],\n        pred['humidity'],\n        pred['dust_density'],\n        pred['co_ppm'],\n        10,  # Proximity_to_Industrial_Areas (consistent with preprocess_dataframe)\n        500  # Population_Density (adjusted to match the input)\n    ], dtype=np.float32)\n    return X_60min.reshape(1, -1)\n\ndef predict_with_vae(X_60min, model_dir=MODEL_DIR):\n    try:\n        # Load the saved models and mappings\n        scaler = joblib.load(os.path.join(model_dir, 'scaler.joblib'))\n        encoder = tf.keras.models.load_model(\n            os.path.join(model_dir, 'encoder_model.keras'),\n            custom_objects={'KLLossLayer': KLLossLayer}\n        )\n        classifier = joblib.load(os.path.join(model_dir, 'classifier.joblib'))\n        with open(os.path.join(model_dir, 'label_mapping.json'), 'r') as f:\n            label_mapping = json.load(f)\n\n        # Create inverse mapping for decoding predictions\n        inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n\n        # Standardize the input data\n        X_60min_scaled = scaler.transform(X_60min)\n\n        # Extract latent features using the encoder\n        X_60min_latent = encoder.predict(X_60min_scaled, verbose=0)\n\n        # Predict using the classifier\n        y_pred = classifier.predict(X_60min_latent)\n\n        # Decode predictions to class names\n        y_labels = [inverse_label_mapping[pred] for pred in y_pred]\n        return y_labels\n    except Exception as e:\n        logger.error(f\"Error in predict_with_vae: {str(e)}\")\n        raise\n\ndef format_predictions(predictions, y_pred_vae, row_count, df_head):\n    \"\"\"Format the prediction results for Telegram message.\"\"\"\n    pred = predictions[-1]\n    air_quality = y_pred_vae[0]  # predict_with_vae returns a list, take the first prediction\n    message = (\n        f\"Dự đoán 1 giờ sau:\\n\"\n        f\"  Temperature: {pred['temperature']:.2f} °C\\n\"\n        f\"  Humidity: {pred['humidity']:.2f} %\\n\"\n        f\"  CO_PPM: {pred['co_ppm']:.6f}\\n\"\n        f\"  Dust_Density: {pred['dust_density']:.6f}\\n\"\n        f\"{'-' * 50}\\n\"\n        f\"Dự đoán chất lượng không khí 1 giờ sau: {air_quality}\"\n    )\n    return message\n\ndef run_inference():\n    \"\"\"Run the inference pipeline and return formatted results.\"\"\"\n    is_ready, message = check_training_status()\n    if not is_ready:\n        return message\n\n    query_api = connect_to_influxdb()\n    tables = fetch_data(query_api)\n\n    df = process_influxdb_data(tables)\n    df = preprocess_dataframe(df)\n    row_count = len(df)\n\n    df_model = df[['temperature', 'humidity', 'dust_density', 'co_ppm',\n                   'Proximity_to_Industrial_Areas', 'Population_Density']].reset_index(drop=True)\n    df_head = df_model.head()\n\n    df_model['timestep'] = df_model.index\n\n    models = load_models_once()\n    scalers = scale_features(df_model)\n    scaled_data = scalers[4:]\n\n    predictions = predict_next_timesteps(models, scalers, scaled_data)\n    pred = predictions[-1]\n\n    X_60min = prepare_rf_input(pred)\n    y_pred_vae = predict_with_vae(X_60min)\n\n    return format_predictions(predictions, y_pred_vae, row_count, df_head)\n\ndef predict_command(update, context):\n    \"\"\"Handle the /predict command from Telegram.\"\"\"\n    try:\n        logger.info(\"Received /predict command\")\n        update.message.reply_text(\"Running inference, please wait...\")\n        result = run_inference()\n        update.message.reply_text(result)\n    except Exception as e:\n        logger.error(f\"Error during inference: {e}\")\n        update.message.reply_text(f\"Error during inference: {str(e)}\")\n\ndef start_command(update, context):\n    \"\"\"Handle the /start command.\"\"\"\n    update.message.reply_text(\"Welcome to the Air Quality Prediction Bot! Use /predict to get air quality forecasts.\")\n\ndef main():\n    \"\"\"Main function to start the Telegram bot.\"\"\"\n    BOT_TOKEN = '7671192466:AAGSHL080boEhvX2mw91vEBmbzGQvTX59kE'\n    updater = Updater(BOT_TOKEN, use_context=True)\n    dp = updater.dispatcher\n\n    dp.add_handler(CommandHandler(\"start\", start_command))\n    dp.add_handler(CommandHandler(\"predict\", predict_command))\n\n    updater.start_polling()\n    logger.info(\"Bot started, listening for commands...\")\n    updater.idle()\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logger.error(f\"Error running bot: {e}\")\n        print(f\"Error running bot: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:38:54.210494Z","iopub.execute_input":"2025-05-14T05:38:54.210762Z"}},"outputs":[{"name":"stderr","text":"2025-05-14 05:38:56.193943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747201136.383795      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747201136.444489      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1747201154.090090     100 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nI0000 00:00:1747201156.277414     118 cuda_dnn.cc:529] Loaded cuDNN version 90300\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1747201175.156735     117 service.cc:148] XLA service 0x7cd09a34c0b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1747201175.157527     117 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1747201175.372278     117 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}